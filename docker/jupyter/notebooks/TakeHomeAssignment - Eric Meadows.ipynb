{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# STANDARD_TYPES\n",
    "INT = \"INT\"\n",
    "FLOAT = \"FLOAT\"\n",
    "NUMERIC = \"NUMERIC\"\n",
    "TEXT = \"TEXT\"\n",
    "TIMESTAMP = \"TIMESTAMP\"\n",
    "DATETIME_PANDAS = \"DATETIME64[NS]\"\n",
    "\n",
    "# STANDARD_DATA_SOURCES\n",
    "PANDAS = \"PANDAS\"\n",
    "\n",
    "DIALECT_MAPPING: Dict[str, Dict[str, str]] = dict(\n",
    "    PANDAS=dict(\n",
    "        OBJECT=TEXT,\n",
    "        DATETIME_PANDAS=TIMESTAMP,\n",
    "        INT64=TEXT,\n",
    "        FLOAT64=TEXT,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def get_tables_by_prefix_separator(data_directory, prefix_separator: str = \"_\") -> List[str]:\n",
    "    potential_tables = [pt.split(prefix_separator)[0] for pt in os.listdir(\"/dataset\")]\n",
    "    tables = set(potential_tables)\n",
    "    return list(tables)\n",
    "\n",
    "def get_csv_data_files_for_table(dataset_directory: str, table: str, prefix_separator: str) -> List[str]:\n",
    "    data_files_glob = f\"{table}{prefix_separator}*.csv\"\n",
    "    data_files = glob.glob(os.path.join(dataset_directory, data_files_glob))\n",
    "    return data_files\n",
    "\n",
    "class DataFrameTools(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_typed_dataframe_from_file(self, data_file: str, date_columns: List[str]):\n",
    "        df = pd.read_csv(data_file, parse_dates=date_columns)\n",
    "        return df\n",
    "\n",
    "    def get_dtypes_dict_from_typed_dataframe(self, typed_df: DataFrame):\n",
    "        dtypes_dict = typed_df.dtypes.to_dict()\n",
    "        for key, value in dtypes_dict.items():\n",
    "            dtypes_dict[key] = str(value).upper()\n",
    "        return dtypes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "from io import StringIO\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "import psycopg2\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Any, Dict\n",
    "\n",
    "DEFAULT_USER = \"postgres\"\n",
    "DEFAULT_PASSWORD = \"password\"\n",
    "DEFAULT_DATABASE = \"take_home\"\n",
    "DEFAULT_SCHEMA = \"eric_meadows\"\n",
    "DEFAULT_HOST = \"database\"\n",
    "\n",
    "DEFAULT_POTENTIAL_NULL_COLUMNS = [\"event_ts\"]\n",
    "\n",
    "\n",
    "\n",
    "TEMPLATE_DIR = os.path.join(os.path.dirname(\"./jinja/\"), \"templates\")\n",
    "TEMPLATE_LOADER = FileSystemLoader(searchpath=TEMPLATE_DIR)\n",
    "TEMPLATE_ENV = Environment(loader=TEMPLATE_LOADER)\n",
    "\n",
    "\n",
    "TABLE_DATE_COLUMNS: Dict[str, List[str]] = dict(\n",
    "    user=[\"event_ts\",],\n",
    "    marketing=[\"event_ts\",]\n",
    ")\n",
    "\n",
    "DROP_AND_CREATE_TABLE_WITH_SCHEMA = \"drop_and_create_table_with_schema.jinja2\"\n",
    "CREATE_SCHEMA_IF_NOT_EXISTS = \"create_schema_if_not_exists.jinja2\"\n",
    "\n",
    "class DatabaseTools(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def load_jinja_query_template(\n",
    "        self,\n",
    "        template_file: str,\n",
    "        params: Dict[str, Any]\n",
    "    ) -> str:\n",
    "        template = TEMPLATE_ENV.get_template(template_file)\n",
    "        rendered_query = template.render(params)\n",
    "        return rendered_query\n",
    "    \n",
    "\n",
    "class PostgresTools(DatabaseTools):\n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str = DEFAULT_HOST,\n",
    "        database: str = DEFAULT_DATABASE,\n",
    "        user: str = DEFAULT_USER,\n",
    "        password: str = DEFAULT_PASSWORD,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(PostgresTools, self).__init__(*args, **kwargs)\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = psycopg2.connect(\n",
    "            host=self.host,\n",
    "            dbname=self.database,\n",
    "            user=self.user,\n",
    "            password=self.password\n",
    "        )\n",
    "        self.dataframe_tools = DataFrameTools()\n",
    "    \n",
    "    def _ensure_schema_present(\n",
    "        self,\n",
    "        schema,\n",
    "    ):\n",
    "        query_params = dict(\n",
    "            schema=schema\n",
    "        ) \n",
    "        query = self.load_jinja_query_template(\n",
    "            CREATE_SCHEMA_IF_NOT_EXISTS,\n",
    "            query_params\n",
    "        )\n",
    "        self._run_query(query)\n",
    "\n",
    "    def _drop_and_create_table_sql(\n",
    "        self,\n",
    "        database: str,\n",
    "        table: str,\n",
    "        dialect: str,\n",
    "        schema_map: Dict[str, str],\n",
    "    ):\n",
    "        dialect_map = DIALECT_MAPPING[dialect]\n",
    "        postgres_table_name = f\"{database}.{table}\"\n",
    "\n",
    "        query_params = dict(\n",
    "            postgres_table_name=postgres_table_name,\n",
    "            mapping=dialect_map,\n",
    "            schema_map=schema_map,\n",
    "        )\n",
    "        query_sql = self.load_jinja_query_template(\n",
    "            DROP_AND_CREATE_TABLE_WITH_SCHEMA,\n",
    "            query_params\n",
    "        )\n",
    "        return query_sql\n",
    "\n",
    "    def _drop_and_create_table(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table: str,\n",
    "        dialect: str,\n",
    "        schema_map: Dict[str, str],\n",
    "    ):\n",
    "        query = self._drop_and_create_table_sql(\n",
    "            schema,\n",
    "            table,\n",
    "            dialect,\n",
    "            schema_map\n",
    "        )\n",
    "        self._run_query(query)\n",
    "    \n",
    "    def _get_file_buffer_without_null_bytes(\n",
    "        self,\n",
    "        data_file: str,\n",
    "        potential_null_columns: List[str] = DEFAULT_POTENTIAL_NULL_COLUMNS\n",
    "    ):\n",
    "        writeable_csv_buffer = StringIO()\n",
    "        pd.read_csv(data_file)\\\n",
    "          .dropna(subset=potential_null_columns)\\\n",
    "          .to_csv(writeable_csv_buffer, index=False, header=False)\n",
    "        writeable_csv_buffer.seek(0)\n",
    "        return writeable_csv_buffer\n",
    "    \n",
    "    def _load_file_into_table(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table: str,\n",
    "        data_file: str,\n",
    "        separator=\",\"\n",
    "    ):\n",
    "        cursor = self.conn.cursor()\n",
    "        writeable_csv_buffer = self._get_file_buffer_without_null_bytes(data_file)\n",
    "        try:\n",
    "            cursor.copy_from(writeable_csv_buffer, f\"{schema}.{table}\", sep=separator, null=\"\")\n",
    "            self.conn.commit()\n",
    "        except Exception as err:\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            raise err\n",
    "        finally:\n",
    "            writeable_csv_buffer.close()\n",
    "            cursor.close()\n",
    "    \n",
    "    def _run_query(self, query):\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        self.conn.commit()\n",
    "        cursor.close()\n",
    "    \n",
    "    def load_files_into_database(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table: str,\n",
    "        data_files: List[str],\n",
    "        dialect: str = PANDAS,\n",
    "    ):\n",
    "        first_data_file = data_files[0]\n",
    "        df = self.dataframe_tools.get_typed_dataframe_from_file(first_data_file, [])\n",
    "        schema_map = self.dataframe_tools.get_dtypes_dict_from_typed_dataframe(df)\n",
    "        \n",
    "        self._ensure_schema_present(schema)\n",
    "\n",
    "        self._drop_and_create_table(\n",
    "            schema,\n",
    "            table,\n",
    "            dialect,\n",
    "            schema_map,\n",
    "        )\n",
    "        for data_file in data_files:\n",
    "            self._load_file_into_table(schema, table, data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for table:  marketing\n",
      "Loading data for table:  user\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "TABLE_DATE_COLUMNS: Dict[str, List[str]] = dict(\n",
    "    user=[\"event_ts\",],\n",
    "    marketing=[\"event_ts\",]\n",
    ")\n",
    "DATASET_DIRECTORY = \"/dataset\"\n",
    "PREFIX_SEPARATOR = \"_\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    postgres_tools = PostgresTools()\n",
    "\n",
    "    required_tables = get_tables_by_prefix_separator(DATASET_DIRECTORY, PREFIX_SEPARATOR)\n",
    "    for table in required_tables:\n",
    "        print(f\"Loading data for table:  {table}\")\n",
    "        data_files = get_csv_data_files_for_table(DATASET_DIRECTORY, table, PREFIX_SEPARATOR)\n",
    "        result = postgres_tools.load_files_into_database(DEFAULT_SCHEMA, table, data_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(f'postgresql://{DEFAULT_USER}:{DEFAULT_PASSWORD}@{DEFAULT_HOST}:5432/{DEFAULT_DATABASE}')\n",
    "\n",
    "USERS_TABLE = \"user\"\n",
    "MARKETING_TABLE = \"marketing\"\n",
    "\n",
    "users_df = pd.read_sql_query(\n",
    "    f\"select * from {DEFAULT_SCHEMA}.{USERS_TABLE}\",\n",
    "    con=engine,\n",
    "    parse_dates=[\"event_ts\"])\n",
    "marketing_df = pd.read_sql_query(\n",
    "    f\"select * from {DEFAULT_SCHEMA}.{MARKETING_TABLE}\",\n",
    "    con=engine,\n",
    "    parse_dates=[\"event_ts\"])\n",
    "marketing_df.length = marketing_df.length.astype(\"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Python]\n",
      "1.  Unique users:  2904\n",
      "2.  Marketing providers:  Facebook, Instagram, Spotify, Snapchat, Inst\n",
      "3.  Most-changed Attribute - Name:  Drinking, Count:  1473\n",
      "4.  Users shown an ad on Snapchat on 2019-07-03:  261\n",
      "5.  Ad shown most to moderates:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"[Python]\")\n",
    "\n",
    "# Question 1\n",
    "unique_user_count = len(users_df.user_id.unique())\n",
    "print(f\"1.  Unique users:  {unique_user_count}\")\n",
    "\n",
    "# Question 2\n",
    "marketing_providers = \", \".join(list(filter(None, marketing_df.provider.unique())) )\n",
    "print(f\"2.  Marketing providers:  {marketing_providers}\")\n",
    "\n",
    "# Question 3\n",
    "change_frequency = users_df.property.value_counts()\n",
    "most_changed_attribute_name = change_frequency.idxmax().title()\n",
    "most_changed_attribute_count = change_frequency.max()\n",
    "print(f\"3.  Most-changed Attribute - Name:  {most_changed_attribute_name}, Count:  {most_changed_attribute_count}\")\n",
    "\n",
    "# Question 4\n",
    "DATE = \"2019-07-03\"\n",
    "PROVIDER = \"Snapchat\"\n",
    "users_show_ad_on_provider_on_given_date = marketing_df\\\n",
    "    .loc[marketing_df.event_ts.dt.strftime(\"%Y-%m-%d\") == DATE]\\\n",
    "    .loc[marketing_df.provider == PROVIDER]\\\n",
    "    .shape[0]\n",
    "print(f\"4.  Users shown an ad on {PROVIDER} on {DATE}:  {users_show_ad_on_provider_on_given_date}\")\n",
    "\n",
    "# Question 5\n",
    "POLITICAL_AFFILIATION = \"moderate\"\n",
    "phone_id_and_political_affiliation = users_df.loc[\n",
    "    users_df.property == \"politics\",\n",
    "    [\"phone_id\", \"value\"]]\\\n",
    "    .dropna(subset=[\"value\"])\\\n",
    "    .rename(columns={\"value\": \"political_affiliation\"})\n",
    "merged_df = pd.merge(\n",
    "    marketing_df,\n",
    "    phone_id_and_political_affiliation,\n",
    "    how=\"inner\",\n",
    "    left_on=\"phone_id\",\n",
    "    right_on=\"phone_id\")\n",
    "most_shown_ad_to_specific_political_affliation = merged_df\\\n",
    "    .loc[merged_df.political_affiliation.str.lower() == POLITICAL_AFFILIATION]\\\n",
    "    .ad_id\\\n",
    "    .value_counts()\\\n",
    "    .idxmax()\n",
    "print(f\"5.  Ad shown most to {POLITICAL_AFFILIATION}s:  {most_shown_ad_to_specific_political_affliation}\")\n",
    "\n",
    "# Work for Question 6\n",
    "question_6_data = marketing_df.groupby(by=[\"ad_id\"])\\\n",
    "    .agg({\"length\": [\"mean\", \"std\"], \"phone_id\": \"nunique\"})\\\n",
    "    .sort_values([\n",
    "        ('phone_id', \"nunique\"),\n",
    "        (\"length\", \"mean\")],\n",
    "    ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SQL]\n",
      "1.  Unique users:  2903\n",
      "2.  Marketing providers:  Facebook, Inst, Instagram, Snapchat, Spotify\n",
      "3.  Most-changed Attribute - Name:  drinking, Count:  1473\n",
      "4.  Users shown an ad on Snapchat on 2019-07-03:  261\n",
      "5.  Ad shown most to moderates:  4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_groups</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>distinct_users</th>\n",
       "      <th>view_time_mean</th>\n",
       "      <th>view_time_percentile_1</th>\n",
       "      <th>view_time_percentile_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>97</td>\n",
       "      <td>1237.164948</td>\n",
       "      <td>-133.553649</td>\n",
       "      <td>2607.883546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>145</td>\n",
       "      <td>1162.662069</td>\n",
       "      <td>-76.760830</td>\n",
       "      <td>2402.084968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>173</td>\n",
       "      <td>1232.901734</td>\n",
       "      <td>-186.057263</td>\n",
       "      <td>2651.860731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>379</td>\n",
       "      <td>1129.005277</td>\n",
       "      <td>-216.018867</td>\n",
       "      <td>2474.029421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>673</td>\n",
       "      <td>1251.282318</td>\n",
       "      <td>-135.784754</td>\n",
       "      <td>2638.349390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_groups ad_id  distinct_users  view_time_mean  view_time_percentile_1  \\\n",
       "0           1    20              97     1237.164948             -133.553649   \n",
       "1           2    17             145     1162.662069              -76.760830   \n",
       "2           3    12             173     1232.901734             -186.057263   \n",
       "3           4     5             379     1129.005277             -216.018867   \n",
       "4           5     0             673     1251.282318             -135.784754   \n",
       "\n",
       "   view_time_percentile_99  \n",
       "0              2607.883546  \n",
       "1              2402.084968  \n",
       "2              2651.860731  \n",
       "3              2474.029421  \n",
       "4              2638.349390  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[SQL]\")\n",
    "\n",
    "# Question 1\n",
    "distinct_users = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  COUNT(DISTINCT(user_id))\n",
    "FROM\n",
    "  {DEFAULT_SCHEMA}.{USERS_TABLE}\"\"\",\n",
    "    con=engine)\n",
    "number_of_distinct_users = distinct_users.iloc[0].loc[\"count\"]\n",
    "print(f\"1.  Unique users:  {number_of_distinct_users}\")\n",
    "\n",
    "# Question 2\n",
    "marketing_providers_list = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  DISTINCT(provider)\n",
    "FROM\n",
    "  {DEFAULT_SCHEMA}.{MARKETING_TABLE}\n",
    "ORDER BY provider\"\"\",\n",
    "    con=engine).values.flatten().tolist()\n",
    "marketing_providers = \", \".join(marketing_providers_list)\n",
    "print(f\"2.  Marketing providers:  {marketing_providers}\")\n",
    "\n",
    "# Question 3\n",
    "most_changed_property = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  property, COUNT(*) AS _count\n",
    "FROM\n",
    "  {DEFAULT_SCHEMA}.{USERS_TABLE}\n",
    "GROUP BY\n",
    "  property\n",
    "ORDER BY\n",
    "  _count DESC\n",
    "LIMIT\n",
    "  1\"\"\",\n",
    "    con=engine).loc[0]\n",
    "print(f\"3.  Most-changed Attribute - Name:  {most_changed_property.property}, Count:  {most_changed_property._count}\")\n",
    "\n",
    "# Question 4\n",
    "DATE = \"2019-07-03\"\n",
    "PROVIDER = \"Snapchat\"\n",
    "users_show_ad_on_provider_on_given_date = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  COUNT(1) AS _count\n",
    "FROM\n",
    "  (\n",
    "    SELECT\n",
    "      *, event_ts::TIMESTAMP AS event_timestamp\n",
    "    FROM\n",
    "      {DEFAULT_SCHEMA}.{MARKETING_TABLE}\n",
    "  ) AS normalized_users\n",
    "WHERE\n",
    "  event_timestamp BETWEEN DATE '{DATE}' AND DATE '{DATE}' + INTERVAL '1 day' AND\n",
    "  provider = '{PROVIDER}'\n",
    "  ;\n",
    "\"\"\",\n",
    "    con=engine).loc[0, \"_count\"]\n",
    "print(f\"4.  Users shown an ad on {PROVIDER} on {DATE}:  {users_show_ad_on_provider_on_given_date}\")\n",
    "\n",
    "# Question 5\n",
    "POLITICS = \"politics\"\n",
    "POLITICAL_AFFILIATION = \"moderate\"\n",
    "most_shown_ad_to_specific_political_affliation = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  ad_id\n",
    "FROM\n",
    "  {DEFAULT_SCHEMA}.{MARKETING_TABLE}\n",
    "WHERE\n",
    "  phone_id IN (\n",
    "  SELECT\n",
    "    phone_id\n",
    "  FROM\n",
    "    (\n",
    "      SELECT\n",
    "        phone_id, property, LOWER(value) AS lower_value\n",
    "      FROM\n",
    "        {DEFAULT_SCHEMA}.{USERS_TABLE}\n",
    "    ) AS standardized_users\n",
    "  WHERE\n",
    "    property = '{POLITICS}' AND\n",
    "    lower_value = '{POLITICAL_AFFILIATION}'\n",
    "  )\n",
    "GROUP BY\n",
    "  ad_id\n",
    "ORDER BY\n",
    "  COUNT(1) DESC\n",
    "LIMIT\n",
    "  1\n",
    "\"\"\",\n",
    "    con=engine).loc[0, \"ad_id\"]\n",
    "print(f\"5.  Ad shown most to {POLITICAL_AFFILIATION}s:  {most_shown_ad_to_specific_political_affliation}\")\n",
    "\n",
    "# Question 6\n",
    "most_successful_ads = pd.read_sql_query(\n",
    "    f\"\"\"\n",
    "SELECT\n",
    "  _user_groups AS \"user_groups\",\n",
    "  ad_id,\n",
    "  _distinct_users AS \"distinct_users\",\n",
    "  _mean AS \"view_time_mean\",\n",
    "  _percent_1 AS \"view_time_percentile_1\",\n",
    "  _percent_99 AS \"view_time_percentile_99\"\n",
    "FROM\n",
    "  (\n",
    "    SELECT\n",
    "      *, MAX(_percent_1) OVER (PARTITION BY _user_groups) AS _max_percent_1\n",
    "    FROM\n",
    "      (\n",
    "        SELECT\n",
    "          ad_id,\n",
    "          _distinct_users,\n",
    "          _mean,\n",
    "          _mean - 2 * _stddev AS _percent_1,\n",
    "          _mean + 2 * _stddev AS _percent_99,\n",
    "          CASE\n",
    "            WHEN _distinct_users BETWEEN 0   AND 100 THEN '1'\n",
    "            WHEN _distinct_users BETWEEN 100 AND 159 THEN '2'\n",
    "            WHEN _distinct_users BETWEEN 160 AND 299 THEN '3'\n",
    "            WHEN _distinct_users BETWEEN 300 AND 499 THEN '4'\n",
    "            ELSE '5'\n",
    "          END AS _user_groups\n",
    "        FROM\n",
    "          (\n",
    "            SELECT\n",
    "              ad_id, COUNT(phone_id) AS _distinct_users, AVG(length::FLOAT) AS _mean, STDDEV(length::FLOAT) AS _stddev\n",
    "            FROM\n",
    "              {DEFAULT_SCHEMA}.{MARKETING_TABLE}\n",
    "            GROUP BY\n",
    "              ad_id\n",
    "          ) AS stats_marketing\n",
    "        ORDER BY\n",
    "          _distinct_users DESC\n",
    "      ) AS stats_marketing_metrics\n",
    "  ) AS stats_marketing_metrics_with_max\n",
    "WHERE\n",
    "  _percent_1 = _max_percent_1\n",
    "\"\"\",\n",
    "    con=engine)\n",
    "most_successful_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table for Markdown in ANSWERS.md:\n",
      "\n",
      "user_groups,ad_id,distinct_users,view_time_mean,view_time_percentile_1,view_time_percentile_99\n",
      "1,20,97,1237.16494845361,-133.553648986325,2607.88354589354\n",
      "2,17,145,1162.66206896552,-76.7608303596678,2402.0849682907\n",
      "3,12,173,1232.90173410405,-186.057263037625,2651.86073124572\n",
      "4,5,379,1129.00527704485,-216.018866596538,2474.02942068625\n",
      "5,0,673,1251.2823179792,-135.78475381805,2638.34938977644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Table for Markdown in ANSWERS.md:\\n\")\n",
    "print(most_successful_ads.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python\n",
    "## Examining the data above, it appears clear that there are 4 ad groups based upon the number of users an ad was shown to\n",
    "## Because of this, there appears to be a winner for each group, and then we will select another\n",
    "### Group 1:  >500 users shown\n",
    "Winner:  ad_id = 0\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 2:  300-499 users shown\n",
    "Winner:  ad_id = 5\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 4:  160-299 users shown\n",
    "Winner:  ad_id = 12\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 3:  100-159 users shown\n",
    "Winner:  ad_id = 14\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 4:  <100 users shown\n",
    "Winner:  ad_id = 20\n",
    "Reason:  mean - 2 std. dev > every other in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
