{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# STANDARD_TYPES\n",
    "INT = \"INT\"\n",
    "FLOAT = \"FLOAT\"\n",
    "NUMERIC = \"NUMERIC\"\n",
    "TEXT = \"TEXT\"\n",
    "TIMESTAMP = \"TIMESTAMP\"\n",
    "DATETIME_PANDAS = \"DATETIME64[NS]\"\n",
    "\n",
    "# STANDARD_DATA_SOURCES\n",
    "PANDAS = \"PANDAS\"\n",
    "\n",
    "DIALECT_MAPPING: Dict[str, Dict[str, str]] = dict(\n",
    "    PANDAS=dict(\n",
    "        OBJECT=TEXT,\n",
    "        DATETIME_PANDAS=TIMESTAMP,\n",
    "        INT64=TEXT,\n",
    "        FLOAT64=TEXT,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "def get_tables_by_prefix_separator(data_directory, prefix_separator: str = \"_\") -> List[str]:\n",
    "    potential_tables = [pt.split(prefix_separator)[0] for pt in os.listdir(\"/dataset\")]\n",
    "    tables = set(potential_tables)\n",
    "    return list(tables)\n",
    "\n",
    "def get_csv_data_files_for_table(dataset_directory: str, table: str, prefix_separator: str) -> List[str]:\n",
    "    data_files_glob = f\"{table}{prefix_separator}*.csv\"\n",
    "    data_files = glob.glob(os.path.join(dataset_directory, data_files_glob))\n",
    "    return data_files\n",
    "\n",
    "class DataFrameTools(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_typed_dataframe_from_file(self, data_file: str, date_columns: List[str]):\n",
    "        df = pd.read_csv(data_file, parse_dates=date_columns)\n",
    "        return df\n",
    "\n",
    "    def get_dtypes_dict_from_typed_dataframe(self, typed_df: DataFrame):\n",
    "        dtypes_dict = typed_df.dtypes.to_dict()\n",
    "        for key, value in dtypes_dict.items():\n",
    "            dtypes_dict[key] = str(value).upper()\n",
    "        return dtypes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "from io import StringIO\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import os\n",
    "import psycopg2\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Any, Dict\n",
    "\n",
    "DEFAULT_USER = \"postgres\"\n",
    "DEFAULT_PASSWORD = \"password\"\n",
    "DEFAULT_DATABASE = \"take_home\"\n",
    "DEFAULT_SCHEMA = \"eric_meadows\"\n",
    "DEFAULT_HOST = \"database\"\n",
    "\n",
    "\n",
    "\n",
    "TEMPLATE_DIR = os.path.join(os.path.dirname(\"./jinja/\"), \"templates\")\n",
    "TEMPLATE_LOADER = FileSystemLoader(searchpath=TEMPLATE_DIR)\n",
    "TEMPLATE_ENV = Environment(loader=TEMPLATE_LOADER)\n",
    "\n",
    "\n",
    "TABLE_DATE_COLUMNS: Dict[str, List[str]] = dict(\n",
    "    user=[\"event_ts\",],\n",
    "    marketing=[\"event_ts\",]\n",
    ")\n",
    "\n",
    "DROP_AND_CREATE_TABLE_WITH_SCHEMA = \"drop_and_create_table_with_schema.jinja2\"\n",
    "CREATE_SCHEMA_IF_NOT_EXISTS = \"create_schema_if_not_exists.jinja2\"\n",
    "\n",
    "class DatabaseTools(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def load_jinja_query_template(\n",
    "        self,\n",
    "        template_file: str,\n",
    "        params: Dict[str, Any]\n",
    "    ) -> str:\n",
    "        template = TEMPLATE_ENV.get_template(template_file)\n",
    "        rendered_query = template.render(params)\n",
    "        return rendered_query\n",
    "    \n",
    "\n",
    "class PostgresTools(DatabaseTools):\n",
    "    def __init__(\n",
    "        self,\n",
    "        host: str = DEFAULT_HOST,\n",
    "        database: str = DEFAULT_DATABASE,\n",
    "        user: str = DEFAULT_USER,\n",
    "        password: str = DEFAULT_PASSWORD,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(PostgresTools, self).__init__(*args, **kwargs)\n",
    "        self.host = host\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = psycopg2.connect(\n",
    "            host=self.host,\n",
    "            dbname=self.database,\n",
    "            user=self.user,\n",
    "            password=self.password\n",
    "        )\n",
    "        self.dataframe_tools = DataFrameTools()\n",
    "    \n",
    "    def _ensure_schema_present(\n",
    "        self,\n",
    "        schema,\n",
    "    ):\n",
    "        query_params = dict(\n",
    "            schema=schema\n",
    "        ) \n",
    "        query = self.load_jinja_query_template(\n",
    "            CREATE_SCHEMA_IF_NOT_EXISTS,\n",
    "            query_params\n",
    "        )\n",
    "        self._run_query(query)\n",
    "\n",
    "    def _drop_and_create_table_sql(\n",
    "        self,\n",
    "        database: str,\n",
    "        table: str,\n",
    "        dialect: str,\n",
    "        schema_map: Dict[str, str],\n",
    "    ):\n",
    "        dialect_map = DIALECT_MAPPING[dialect]\n",
    "        postgres_table_name = f\"{database}.{table}\"\n",
    "\n",
    "        query_params = dict(\n",
    "            postgres_table_name=postgres_table_name,\n",
    "            mapping=dialect_map,\n",
    "            schema_map=schema_map,\n",
    "        )\n",
    "        query_sql = self.load_jinja_query_template(\n",
    "            DROP_AND_CREATE_TABLE_WITH_SCHEMA,\n",
    "            query_params\n",
    "        )\n",
    "        return query_sql\n",
    "\n",
    "    def _drop_and_create_table(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table: str,\n",
    "        dialect: str,\n",
    "        schema_map: Dict[str, str],\n",
    "    ):\n",
    "        query = self._drop_and_create_table_sql(\n",
    "            schema,\n",
    "            table,\n",
    "            dialect,\n",
    "            schema_map\n",
    "        )\n",
    "        self._run_query(query)\n",
    "    \n",
    "    def _get_file_buffer_without_null_bytes(\n",
    "        self,\n",
    "        data_file\n",
    "    ):\n",
    "        writeable_csv_buffer = StringIO()\n",
    "        # Circumvent the \n",
    "        pd.read_csv(data_file).to_csv(writeable_csv_buffer, index=False, header=False)\n",
    "        writeable_csv_buffer.seek(0)\n",
    "        return writeable_csv_buffer\n",
    "    \n",
    "    def _load_file_into_table(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table: str,\n",
    "        data_file: str,\n",
    "        separator=\",\"\n",
    "    ):\n",
    "        cursor = self.conn.cursor()\n",
    "        writeable_csv_buffer = self._get_file_buffer_without_null_bytes(data_file)\n",
    "        try:\n",
    "            cursor.copy_from(writeable_csv_buffer, f\"{schema}.{table}\", sep=separator, null=\"\")\n",
    "            self.conn.commit()\n",
    "        except Exception as err:\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            raise err\n",
    "        finally:\n",
    "            writeable_csv_buffer.close()\n",
    "            cursor.close()\n",
    "    \n",
    "    def _run_query(self, query):\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        self.conn.commit()\n",
    "        cursor.close()\n",
    "    \n",
    "    def load_files_into_database(\n",
    "        self,\n",
    "        schema: str,\n",
    "        table: str,\n",
    "        data_files: List[str],\n",
    "        dialect: str = PANDAS,\n",
    "    ):\n",
    "        first_data_file = data_files[0]\n",
    "        df = self.dataframe_tools.get_typed_dataframe_from_file(first_data_file, [])\n",
    "        schema_map = self.dataframe_tools.get_dtypes_dict_from_typed_dataframe(df)\n",
    "        \n",
    "        self._ensure_schema_present(schema)\n",
    "\n",
    "        self._drop_and_create_table(\n",
    "            schema,\n",
    "            table,\n",
    "            dialect,\n",
    "            schema_map,\n",
    "        )\n",
    "        for data_file in data_files:\n",
    "            self._load_file_into_table(schema, table, data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for table:  user\n",
      "Loading data for table:  marketing\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "TABLE_DATE_COLUMNS: Dict[str, List[str]] = dict(\n",
    "    user=[\"event_ts\",],\n",
    "    marketing=[\"event_ts\",]\n",
    ")\n",
    "DATASET_DIRECTORY = \"/dataset\"\n",
    "PREFIX_SEPARATOR = \"_\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    postgres_tools = PostgresTools()\n",
    "\n",
    "    required_tables = get_tables_by_prefix_separator(DATASET_DIRECTORY, PREFIX_SEPARATOR)\n",
    "    for table in required_tables:\n",
    "        print(f\"Loading data for table:  {table}\")\n",
    "        data_files = get_csv_data_files_for_table(DATASET_DIRECTORY, table, PREFIX_SEPARATOR)\n",
    "        result = postgres_tools.load_files_into_database(DEFAULT_SCHEMA, table, data_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(f'postgresql://{DEFAULT_USER}:{DEFAULT_PASSWORD}@{DEFAULT_HOST}:5432/{DEFAULT_DATABASE}')\n",
    "\n",
    "USERS_TABLE = \"user\"\n",
    "MARKETING_TABLE = \"marketing\"\n",
    "\n",
    "users_df = pd.read_sql_query(\n",
    "    f\"select * from {DEFAULT_SCHEMA}.{USERS_TABLE}\",\n",
    "    con=engine,\n",
    "    parse_dates=[\"event_ts\"])\\\n",
    "    .dropna(subset=[\"event_ts\"])\n",
    "marketing_df = pd.read_sql_query(\n",
    "    f\"select * from {DEFAULT_SCHEMA}.{MARKETING_TABLE}\",\n",
    "    con=engine,\n",
    "    parse_dates=[\"event_ts\"])\\\n",
    "    .dropna(subset=[\"event_ts\"])\n",
    "marketing_df.length = marketing_df.length.astype(\"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  Unique users:  2904\n"
     ]
    }
   ],
   "source": [
    "unique_user_count = len(users_df.user_id.unique())\n",
    "print(f\"1.  Unique users:  {unique_user_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.  Marketing providers:  Facebook, Instagram, Spotify, Snapchat, Inst\n"
     ]
    }
   ],
   "source": [
    "marketing_providers = \", \".join(list(filter(None, marketing_df.provider.unique())) )\n",
    "print(f\"2.  Marketing providers:  {marketing_providers}\")\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.  Most-changed Attribute - Name:  {most_changed_attribute_name}, Count:  {most_changed_attribute_count}\n"
     ]
    }
   ],
   "source": [
    "change_frequency = users_df.property.value_counts()\n",
    "most_changed_attribute_name = change_frequency.idxmax().title()\n",
    "most_changed_attribute_count = change_frequency.max()\n",
    "\n",
    "print(\"3.  Most-changed Attribute - Name:  {most_changed_attribute_name}, Count:  {most_changed_attribute_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.  Users shown an ad on Snapchat on 2019-07-03:  261\n"
     ]
    }
   ],
   "source": [
    "DATE = \"2019-07-03\"\n",
    "PROVIDER = \"Snapchat\"\n",
    "users_show_ad_on_provider_on_given_date = marketing_df\\\n",
    "    .loc[marketing_df.event_ts.dt.strftime(\"%Y-%m-%d\") == DATE]\\\n",
    "    .loc[marketing_df.provider == PROVIDER]\\\n",
    "    .shape[0]\n",
    "print(f\"4.  Users shown an ad on {PROVIDER} on {DATE}:  {users_show_ad_on_provider_on_given_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.  Ad shown most to moderates:  4\n"
     ]
    }
   ],
   "source": [
    "POLITICAL_AFFILIATION = \"moderate\"\n",
    "\n",
    "phone_id_and_political_affiliation = users_df.loc[\n",
    "    users_df.property == \"politics\",\n",
    "    [\"phone_id\", \"value\"]]\\\n",
    "    .dropna(subset=[\"value\"])\\\n",
    "    .rename(columns={\"value\": \"political_affiliation\"})\n",
    "merged_df = pd.merge(\n",
    "    marketing_df,\n",
    "    phone_id_and_political_affiliation,\n",
    "    how=\"inner\",\n",
    "    left_on=\"phone_id\",\n",
    "    right_on=\"phone_id\")\n",
    "\n",
    "most_shown_ad_to_specific_political_affliation = merged_df\\\n",
    "    .loc[merged_df.political_affiliation.str.lower() == POLITICAL_AFFILIATION]\\\n",
    "    .ad_id\\\n",
    "    .value_counts()\\\n",
    "    .idxmax()\n",
    "print(f\"5.  Ad shown most to {POLITICAL_AFFILIATION}s:  {most_shown_ad_to_specific_political_affliation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">length</th>\n",
       "      <th>phone_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1187.477852</td>\n",
       "      <td>698.685703</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1194.655267</td>\n",
       "      <td>687.216389</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200.760522</td>\n",
       "      <td>680.341966</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1251.282318</td>\n",
       "      <td>693.533536</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1189.101040</td>\n",
       "      <td>689.615890</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1129.005277</td>\n",
       "      <td>672.512072</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1156.726519</td>\n",
       "      <td>688.105589</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1166.478022</td>\n",
       "      <td>722.131942</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1167.659341</td>\n",
       "      <td>712.691252</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1146.465465</td>\n",
       "      <td>703.231707</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1163.862434</td>\n",
       "      <td>682.753446</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1115.412088</td>\n",
       "      <td>692.251607</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1232.901734</td>\n",
       "      <td>709.479499</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1207.069767</td>\n",
       "      <td>708.421733</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1160.012270</td>\n",
       "      <td>683.031035</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1240.331210</td>\n",
       "      <td>700.184256</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1136.241611</td>\n",
       "      <td>672.837594</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1162.662069</td>\n",
       "      <td>619.711450</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1136.521127</td>\n",
       "      <td>694.674818</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1280.368794</td>\n",
       "      <td>685.893238</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1237.164948</td>\n",
       "      <td>685.359299</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1174.693182</td>\n",
       "      <td>724.606597</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1151.413333</td>\n",
       "      <td>686.158088</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length             phone_id\n",
       "              mean         std  nunique\n",
       "ad_id                                  \n",
       "1      1187.477852  698.685703      639\n",
       "4      1194.655267  687.216389      635\n",
       "2      1200.760522  680.341966      612\n",
       "0      1251.282318  693.533536      605\n",
       "3      1189.101040  689.615890      592\n",
       "5      1129.005277  672.512072      351\n",
       "8      1156.726519  688.105589      346\n",
       "6      1166.478022  722.131942      337\n",
       "9      1167.659341  712.691252      335\n",
       "7      1146.465465  703.231707      313\n",
       "15     1163.862434  682.753446      180\n",
       "10     1115.412088  692.251607      179\n",
       "12     1232.901734  709.479499      166\n",
       "18     1207.069767  708.421733      166\n",
       "16     1160.012270  683.031035      155\n",
       "11     1240.331210  700.184256      154\n",
       "13     1136.241611  672.837594      141\n",
       "17     1162.662069  619.711450      140\n",
       "19     1136.521127  694.674818      140\n",
       "14     1280.368794  685.893238      139\n",
       "20     1237.164948  685.359299       92\n",
       "21     1174.693182  724.606597       88\n",
       "22     1151.413333  686.158088       75"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing_df.groupby(by=[\"ad_id\"])\\\n",
    "    .agg({\"length\": [\"mean\", \"std\"], \"phone_id\": \"nunique\"})\\\n",
    "    .sort_values([\n",
    "        ('phone_id', \"nunique\"),\n",
    "        (\"length\", \"mean\")],\n",
    "    ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the data above, it appears clear that there are 4 ad groups based upon the number of users an ad was shown to\n",
    "## Because of this, there appears to be a winner for each group, and then we will select another\n",
    "### Group 1:  >500 users shown\n",
    "Winner:  ad_id = 0\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 2:  300-499 users shown\n",
    "Winner:  ad_id = 5\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 4:  160-299 users shown\n",
    "Winner:  ad_id = 12\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 3:  100-159 users shown\n",
    "Winner:  ad_id = 14\n",
    "Reason:  mean - 2 std. dev > every other in the group\n",
    "### Group 4:  <100 users shown\n",
    "Winner:  ad_id = 20\n",
    "Reason:  mean - 2 std. dev > every other in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
